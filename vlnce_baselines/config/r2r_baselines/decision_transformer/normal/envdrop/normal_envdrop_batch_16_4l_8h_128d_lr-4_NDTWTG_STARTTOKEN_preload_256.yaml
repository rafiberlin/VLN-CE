#BEST Val Seen only 0.15 SPL, convergence is anything below 0.30 Mean Loss
BASE_TASK_CONFIG_PATH: habitat_extensions/config/vlnce_task_aug.yaml
TRAINER_NAME: decision_transformer # recollect_trainer, dagger
SIMULATOR_GPU_IDS: [0]
TORCH_GPU_ID: 0
NUM_ENVIRONMENTS: 6 #Looks like the sweet spot for dataset creation is 6 envs. If you use more, we might wait too long to reload the next batch....
TENSORBOARD_DIR: data/tensorboard-logdir/normal/envdrop_batch_16_4l_8h_128d_lr-4_NDTWTG_STARTTOKEN_preload_256
CHECKPOINT_FOLDER: data/checkpoints/normal/envdrop_batch_16_4l_8h_128d_lr-4_NDTWTG_STARTTOKEN_preload_256
EVAL_CKPT_PATH_DIR: data/checkpoints/normal/envdrop_batch_16_4l_8h_128d_lr-4_NDTWTG_STARTTOKEN_preload_256
RESULTS_DIR: data/checkpoints/normal/envdrop_batch_16_4l_8h_128d_lr-4_NDTWTG_STARTTOKEN_preload_256/evals/val_seen
MULTIPROCESSING: forkserver  # default is forkserver, Set to 'spawn' when val_seenging with Pycharm,
ENV_NAME: VLNCEDecisionTransformerEnv


INFERENCE:
  SPLIT: test
  USE_CKPT_CONFIG: False
  SAMPLE: False
  CKPT_PATH: data/checkpoints/normal/envdrop_batch_16_4l_8h_128d_lr-4_NDTWTG_STARTTOKEN_preload_256/ckpt.269.pth
  PREDICTIONS_FILE: predictions.json
  FORMAT: r2r


EVAL:
  USE_CKPT_CONFIG: False
  SPLIT: val_seen # val_unseen
  EPISODE_COUNT: -1

IL:
  optimizer: torch.optim.RAdam # for example, "torch.optim.Adam" or torch.optim.Adam
  preload_dataloader_size: 256
  epochs: 51
  batch_size: 16
  checkpoint_frequency: 3
  lr: 2.5e-4
  load_from_ckpt: True # To train from a previous checkpoint
  ckpt_to_load: /media/rafi/Samsung_T5/_DATASETS/VLNCE/data/checkpoints/normal/envdrop_batch_16_4l_8h_128d_lr-4_NDTWTG_STARTTOKEN_preload_256/_pretrained/ckpt.50.pth # REPLACE

  RECOLLECT_TRAINER:
    gt_file:
      data/datasets/R2R_VLNCE_v1-3_preprocessed/{split}/{split}_gt.json.gz
  DECISION_TRANSFORMER:
    sensor_uuid: distance_left
    POINT_GOAL_NAV_REWARD:
      step_penalty: -0.01
      success: 1.0
    SPARSE_REWARD:
      step_penalty: -0.01
      success: 1.0
    NDTW_REWARD:
      step_penalty: -0.01
      success: 1.0
  DAGGER:
    repeat_dataset: 1
    iterations: 1
    update_size: 157232
    p: 1.0
    preload_lmdb_features: True
    lmdb_commit_frequency: 500
    lmdb_features_dir: data/trajectories_dirs/decision_transformer_aug_v3_PGR_1_-0.01_SR_1_-0.01_NDTWR_1_-0.01/trajectories.lmdb


MODEL:
  DECISION_TRANSFORMER:
    net: DecisionTransformerNet # DecisionTransformerNet or DecisionTransformerEnhancedNet
    use_extra_start_token: True
    reward_type: ndtw_reward_to_go # point_nav_reward_to_go, sparse_reward_to_go, point_nav_reward, sparse_reward, ndtw_reward, ndtw_reward_to_go
    n_layer: 4
    n_head: 8
    hidden_dim: 128 # The dimensions for the State, Reward and actions embeddings and in the GPT Backbne
  policy_name: DecisionTransformerPolicy
  PROGRESS_MONITOR:
    use: True
