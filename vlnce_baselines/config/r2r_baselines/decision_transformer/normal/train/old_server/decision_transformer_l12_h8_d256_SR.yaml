BASE_TASK_CONFIG_PATH: habitat_extensions/config/vlnce_task.yaml
TRAINER_NAME: decision_transformer # recollect_trainer, dagger
SIMULATOR_GPU_IDS: [1]
TORCH_GPU_ID: 0
NUM_ENVIRONMENTS: 8 #Only use one environment for Decision Transformer! An this is probably not working with MUlti Sim GPU
TENSORBOARD_DIR: data/tensorboard_dirs/decision_transformer
CHECKPOINT_FOLDER: data/checkpoints/decision_transformer
EVAL_CKPT_PATH_DIR: data/checkpoints/decision_transformer
RESULTS_DIR: data/checkpoints/decision_transformer/evals
MULTIPROCESSING: forkserver  # default is forkserver, Set to 'spawn' when debugging with Pycharm,
ENV_NAME: VLNCEDecisionTransformerEnv


EVAL:
  USE_CKPT_CONFIG: False
  SPLIT: val_unseen
  EPISODE_COUNT: -1

IL:
  epochs: 10
  batch_size: 8
  checkpoint_frequency: 1

  RECOLLECT_TRAINER:
    gt_file:
      data/datasets/R2R_VLNCE_v1-3_preprocessed/{split}/{split}_gt.json.gz
  DECISION_TRANSFORMER:
    sensor_uuid: distance_left
    POINT_GOAL_NAV_REWARD:
      step_penalty: -0.01
      success: 1.0
    SPARSE_REWARD:
      step_penalty: 0
      success: 1.0
  DAGGER:
    iterations: 1
    update_size: 10819
    p: 1.0
    preload_lmdb_features: True
    lmdb_commit_frequency: 500
    lmdb_features_dir: data/trajectories_dirs/decision_transformer_PR1.00-0.01_SR1.00_0.00/trajectories.lmdb


MODEL:
  DECISION_TRANSFORMER:
    reward_type: SPARSE_REWARD # POINT_GOAL_NAV_REWARD or SPARSE_REWARD
    n_layer: 12
    n_head: 8
    hidden_dim: 256 # The dimensions for the State, Reward and actions embeddings and in the GPT Backbne
  policy_name: DecisionTransformerPolicy
  PROGRESS_MONITOR:
    use: True
