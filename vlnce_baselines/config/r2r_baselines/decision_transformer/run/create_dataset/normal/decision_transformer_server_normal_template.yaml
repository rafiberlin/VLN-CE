BASE_TASK_CONFIG_PATH: habitat_extensions/config/vlnce_task_aug.yaml #vlnce_task.yaml,  vlnce_task_50.yaml, vlnce_task_50_plus.yaml, vlnce_task_aug_50.yaml, vlnce_task_aug_50_plus.yaml
TRAINER_NAME: decision_transformer # recollect_trainer, dagger
SIMULATOR_GPU_IDS: [0]
TORCH_GPU_ID: 0
NUM_ENVIRONMENTS: 15 #Only use one environment for Decision Transformer! An this is probably not working with MUlti Sim GPU
#TENSORBOARD_DIR: data/tensorboard_dirs/decision_transformer_server_template
TENSORBOARD_DIR: /cache/tensorboard-logdir/decision_transformer_server_template
CHECKPOINT_FOLDER: data/checkpoints/decision_transformer_server_template
EVAL_CKPT_PATH_DIR: data/checkpoints/decision_transformer_server_template
RESULTS_DIR: data/checkpoints/decision_transformer_server_template/evals/val_seen
MULTIPROCESSING: forkserver  # default is forkserver, Set to 'spawn' when debugging with Pycharm,
ENV_NAME: VLNCEDecisionTransformerEnv

DATASET:
  TYPE: VLN-CE-v1
  SPLIT: train
  DATA_PATH: data/datasets/R2R_VLNCE_v1-3_preprocessed/{split}/{split}.json.gz
  SCENES_DIR: data/scene_datasets/

EVAL:
  USE_CKPT_CONFIG: False
  SPLIT: val_seen
  EPISODE_COUNT: -1

IL:
  dataload_workers: 5 # You need to try different values if dataloading is a bottleneck.
  optimizer: torch.optim.RAdam # Original: torch.optim.Adam. Possible torch.optim.AdamW, torch.optim.RAdam
  preload_dataloader_size: 128 #Original 100
  epochs: 30
  batch_size: 8
  checkpoint_frequency: 1
  lr: 1.5e-4
  load_from_ckpt: False # To train from a previous checkpoint
  ckpt_to_load: /data/VLNCE/data/checkpoint/decision_transformer_server_template/LAST.pth # REPLACE
  RECOLLECT_TRAINER:
    gt_file:
      data/datasets/R2R_VLNCE_v1-3_preprocessed/{split}/{split}_gt.json.gz
  DECISION_TRANSFORMER:
    recompute_reward: True # allows to apply step penalty and end reward on the fly without recaching the whole dataset...
    sensor_uuid: distance_left
    POINT_GOAL_NAV_REWARD:
      step_penalty: -0.2
      success: 1.0
    SPARSE_REWARD:
      step_penalty: -0.01
      success: 1.0
    NDTW_REWARD:
      step_penalty: -0.01
      success: 1.0
  DAGGER:
    repeat_dataset: 1
    iterations: 1
    update_size: 10819 # Original size: 10819, EnvDrop+Train: 157232
    p: 1.0
    preload_lmdb_features: False
    lmdb_commit_frequency: 500
    lmdb_features_dir: data/trajectories_dirs/_normal_gold_/trajectories.lmdb

MODEL:
  DECISION_TRANSFORMER:
    # Possible values: DecisionTransformerNet, DecisionTransformerEnhancedNet
    # DecisionTransformerWithAttendedInstructionsNet or  FullDecisionTransformerNet
    net: DecisionTransformerNet
    use_extra_start_token: True
    reward_type: point_nav_reward_to_go # point_nav_reward_to_go, sparse_reward_to_go, point_nav_reward, sparse_reward, ndtw_reward, ndtw_reward_to_go
    n_layer: 3
    n_head: 8
    hidden_dim: 128 # The dimensions for the State, Reward and actions embeddings and in the GPT Backbne
    use_transformer_encoded_instruction: False
    ATTENTION_LAYER: # only for DecisionTransformerWithAttendedInstructionsNet
      n_head: 8
      n_embd: 256
      use_instruction_state_embeddings: True
      use_depth_influenced_text: True
      use_rgb_influenced_text: True
      use_state_attended_text: True # Here maybe you shouldn t add the time embeddings? As you already did it in the
      use_rgb_state_embeddings: True
      use_depth_state_embeddings: True
    ENCODER: # only for FullDecisionTransformerNet
      n_layer: 3
      n_head: 8
  policy_name: DecisionTransformerPolicy
  PROGRESS_MONITOR: # This is only used to create the datasets and the reward signals
    use: True
